{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 19:51:28.213094: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   61/34007 [..............................] - ETA: 2:46:15 - loss: 0.8783"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/zhangwenbo/testProjects/laplace/future_taobao_lstm_quant.ipynb 单元格 1\u001b[0m line \u001b[0;36m<cell line: 103>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhangwenbo/testProjects/laplace/future_taobao_lstm_quant.ipynb#W0sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/zhangwenbo/testProjects/laplace/future_taobao_lstm_quant.ipynb#W0sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39m# 使用生成器训练模型\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/zhangwenbo/testProjects/laplace/future_taobao_lstm_quant.ipynb#W0sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit_generator(train_generator, epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/zhangwenbo/testProjects/laplace/future_taobao_lstm_quant.ipynb#W0sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mmodel_taobao_lstm_quant.h5\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/keras/src/engine/training.py:2810\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2798\u001b[0m \u001b[39m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[1;32m   2799\u001b[0m \n\u001b[1;32m   2800\u001b[0m \u001b[39mDEPRECATED:\u001b[39;00m\n\u001b[1;32m   2801\u001b[0m \u001b[39m  `Model.fit` now supports generators, so there is no longer any need to\u001b[39;00m\n\u001b[1;32m   2802\u001b[0m \u001b[39m  use this endpoint.\u001b[39;00m\n\u001b[1;32m   2803\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2804\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   2805\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m`Model.fit_generator` is deprecated and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2806\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwill be removed in a future version. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2807\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2808\u001b[0m     stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m   2809\u001b[0m )\n\u001b[0;32m-> 2810\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m   2811\u001b[0m     generator,\n\u001b[1;32m   2812\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m   2813\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m   2814\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2815\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   2816\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[1;32m   2817\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m   2818\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[1;32m   2819\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[1;32m   2820\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   2821\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   2822\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   2823\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m   2824\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[1;32m   2825\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1463\u001b[0m   )\n\u001b[1;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense,GRU,Dropout, Bidirectional\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.utils import Sequence\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "data_clean = pd.read_csv(\"future_taobao_ss2401_tick.csv\")\n",
    "\n",
    "\n",
    "# 1. 数据加载和预处理\n",
    "data_clean['last_price'] = pd.to_numeric(data_clean['last_price'], errors='coerce')\n",
    "\n",
    "price_features = ['highest','lowest','bid_price1','ask_price1','bid_price2','ask_price2','bid_price3','ask_price3','bid_price4','ask_price4','bid_price5','ask_price5']\n",
    "for feature in price_features:\n",
    "    data_clean[feature + '_diff'] = data_clean['last_price'] - data_clean[feature]\n",
    "\n",
    "data_clean['datetime'] = pd.to_datetime(data_clean['datetime'])\n",
    "\n",
    "data_clean['last_price_diff'] = data_clean['last_price'].diff()\n",
    "\n",
    "# Define label\n",
    "data_clean['label'] = data_clean['last_price'].shift(-100) - data_clean['last_price']\n",
    "\n",
    "\n",
    "features = ['last_price_diff', 'volume','bid_volume1','bid_volume2','bid_volume3','bid_volume4','bid_volume5','ask_volume1','ask_volume2','ask_volume3','ask_volume4','ask_volume5'] + [f + '_diff' for f in price_features]\n",
    "\n",
    "# 3. 分割数据\n",
    "\n",
    "# Now you can filter the data between two dates\n",
    "train_data = data_clean[(data_clean['datetime'] >= '2023-08-31 09:00:00') & \n",
    "                        (data_clean['datetime'] < '2023-11-27 09:00:00')]\n",
    "\n",
    "test_data = data_clean[(data_clean['datetime'] >= '2023-11-27 09:00:00') & \n",
    "                        (data_clean['datetime'] < '2023-11-29 09:00:00')]\n",
    "\n",
    "\n",
    "# 初始化归一化器\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "train_data[features] = scaler.fit_transform(train_data[features])\n",
    "\n",
    "\n",
    "# 将 DataFrame 转换为 NumPy 数组\n",
    "X_train = np.array(train_data[features])\n",
    "y_train = np.array(train_data['label'])\n",
    "\n",
    "# 删除 NaN 值\n",
    "mask = ~np.isnan(X_train).any(axis=1)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# 首先，确保 X_train 和 X_test 没有 NaN 值\n",
    "X_train = X_train[~np.isnan(X_train).any(axis=1)]\n",
    "y_train = y_train[~np.isnan(X_train).any(axis=1)]\n",
    "\n",
    "class TimeseriesGenerator(Sequence):\n",
    "    def __init__(self, data, labels, length, stride=1, batch_size=32):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.length = length\n",
    "        self.stride = stride\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil((len(self.data) - self.length) / float(self.stride * self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "\n",
    "        start = idx * self.batch_size * self.stride\n",
    "        end = start + self.batch_size * self.stride + self.length\n",
    "\n",
    "        for i in range(start, min(end, len(self.data) - self.length), self.stride):\n",
    "            batch_x.append(self.data[i: i + self.length])\n",
    "            batch_y.append(self.labels[i + self.length])\n",
    "\n",
    "        return np.array(batch_x), np.array(batch_y)\n",
    "\n",
    "# 定义时间步长和步长\n",
    "time_steps = 300\n",
    "stride = 1  # 增加步长以减少内存使用\n",
    "\n",
    "# 创建数据生成器\n",
    "train_generator = TimeseriesGenerator(X_train, y_train, length=time_steps, stride=stride, batch_size=32)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, input_shape=(time_steps, X_train.shape[1])))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "\n",
    "# 使用生成器训练模型\n",
    "model.fit_generator(train_generator, epochs=3)\n",
    "model.save('model_taobao_lstm_quant.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('model_taobao_lstm_quant.h5')\n",
    "\n",
    "def predict_next_move(tick, model, time_steps,historical_data,scaler):\n",
    "    # 将新的 tick 数据追加到历史数据中\n",
    "    historical_data = pd.concat([historical_data, pd.DataFrame([tick])], ignore_index=True)\n",
    "\n",
    "    # 检查是否有足够的数据来计算滚动和EWM特征\n",
    "    if len(historical_data) >= time_steps+20:\n",
    "\n",
    "\n",
    "        for feature in price_features:\n",
    "            historical_data[feature + '_diff'] = historical_data['last_price'] - historical_data[feature]\n",
    "\n",
    "        historical_data['last_price_diff'] = historical_data['last_price'].diff()\n",
    "\n",
    "\n",
    "        data_for_scaling = historical_data[features].dropna()\n",
    "\n",
    "        # 选择最近的time_steps行用于归一化\n",
    "        data_to_scale = data_for_scaling.tail(time_steps)\n",
    "\n",
    "        # 归一化\n",
    "        scaled_data = scaler.fit_transform(data_to_scale)\n",
    "        \n",
    "\n",
    "        # 使用归一化的数据创建模型输入\n",
    "        X_new = scaled_data.reshape(1, time_steps, len(features))\n",
    "\n",
    "\n",
    "        # 检查X_new是否包含NaN值\n",
    "        if np.isnan(X_new).any():\n",
    "            return None, historical_data\n",
    "        else:\n",
    "            # 进行预测\n",
    "            prediction_proba = model.predict(X_new,verbose=0)\n",
    "            probability_of_one = prediction_proba[0][0]\n",
    "\n",
    "            return probability_of_one, historical_data\n",
    "    else:\n",
    "        # 数据不足以进行预测\n",
    "        return None, historical_data\n",
    "\n",
    "\n",
    "\n",
    "# Initialize historical_data with the correct column names and types if necessary\n",
    "historical_data = pd.DataFrame()\n",
    "scaled_historical_data = pd.DataFrame()\n",
    "\n",
    "initial_funds = 100000\n",
    "funds = initial_funds\n",
    "stock_quantity = 0\n",
    "stock_price = 0\n",
    "buy_threshold = 5\n",
    "# sold_threshold = 0.4\n",
    "transactions = []\n",
    "minute_count = 0\n",
    "\n",
    "for idx, row in test_data.iterrows():\n",
    "    current_probability, historical_data = predict_next_move(row, model,time_steps,historical_data,scaler) \n",
    "    # print(current_probability)\n",
    "    # print(row['trade_time'])\n",
    "    if current_probability is not None:\n",
    "        current_price = row['last_price']\n",
    "        minute_count = minute_count + 1\n",
    "        if current_probability > buy_threshold and stock_quantity == 0 :\n",
    "            print(current_probability)\n",
    "            stock_quantity = funds // current_price\n",
    "            funds -= stock_quantity * current_price\n",
    "            fee = stock_quantity * 2\n",
    "            funds -= fee\n",
    "            stock_price = current_price\n",
    "            buy_price = current_price  # 记录买入价格\n",
    "            print(f\"Time: {row['datetime']} - Action: BUY at {current_price}, Quantity: {stock_quantity},Funds:{funds}\")\n",
    "            transactions.append({\n",
    "                'action': 'buy',\n",
    "                'time': row['datetime'],\n",
    "                'price': current_price,\n",
    "                'quantity': stock_quantity,\n",
    "                'funds_remaining': funds\n",
    "            })\n",
    "            minute_count = 0\n",
    "        elif minute_count > 100 and stock_quantity > 0:\n",
    "            print(current_probability)\n",
    "            funds += stock_quantity * current_price\n",
    "            price_diff = current_price - buy_price  # 计算价格差异\n",
    "            # 判断价格差异是涨、跌还是平\n",
    "            if price_diff > 0:\n",
    "                direction = '涨'\n",
    "            elif price_diff < 0:\n",
    "                direction = '跌'\n",
    "            else:\n",
    "                direction = '平'\n",
    "            print(f\"Time: {row['datetime']} - Action: SELL at {current_price}, Quantity: {stock_quantity},Funds:{funds}, Price Change: {direction}\")\n",
    "            transactions.append({\n",
    "                'action': 'sell',\n",
    "                'time': row['datetime'],\n",
    "                'price': current_price,\n",
    "                'quantity': stock_quantity,\n",
    "                'funds_remaining': funds,\n",
    "                'price_change': direction\n",
    "            })\n",
    "            stock_quantity = 0\n",
    "            buy_price = 0  # 重置买入价格为0\n",
    "\n",
    "    \n",
    "if len(transactions)>0:\n",
    "    transactions_df = pd.DataFrame(transactions)\n",
    "    print(transactions_df['price_change'].value_counts())\n",
    "transactions_df.to_csv('transactions_tick.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from datetime import datetime, time\n",
    "model = load_model('model_taobao_lstm.h5')\n",
    "\n",
    "def parse_time_range(time_range_str):\n",
    "    \"\"\"解析时间范围字符串并返回时间对象的开始和结束时间\"\"\"\n",
    "    start_str, end_str = time_range_str.split('-')\n",
    "    start_time = datetime.strptime(start_str, \"%H:%M\").time()\n",
    "    end_time = datetime.strptime(end_str, \"%H:%M\").time()\n",
    "    return start_time, end_time\n",
    "\n",
    "def is_time_in_ranges(time_to_check, time_ranges):\n",
    "    \"\"\"判断给定时间是否在时间范围数组内\"\"\"\n",
    "    for time_range in time_ranges:\n",
    "        start_time, end_time = parse_time_range(time_range)\n",
    "        if start_time <= time_to_check <= end_time:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def prepare_data_for_prediction(test_data, time_steps, scaler):\n",
    "\n",
    "    # 使用归一化\n",
    "    scaled_data = scaler.fit_transform(test_data[features].dropna())\n",
    "\n",
    "    # 重塑数据以适应模型\n",
    "    X = np.array([scaled_data[i:i+time_steps] for i in range(len(scaled_data)-time_steps+1)])\n",
    "    return X\n",
    "\n",
    "\n",
    "# 定义时间范围数组\n",
    "notrade_time = [\"11:20-11:30\",\"14:50-15:00\",\"0:30-1:00\"]\n",
    "# 准备数据\n",
    "X_test = prepare_data_for_prediction(test_data, time_steps, scaler)\n",
    "# 批量预测\n",
    "predictions = model.predict(X_test, verbose=0)\n",
    "# 确保test_data的索引与predictions对齐\n",
    "aligned_test_data = test_data.iloc[time_steps - 1:]\n",
    "\n",
    "\n",
    "initial_funds = 100000\n",
    "funds = initial_funds\n",
    "stock_quantity = 0\n",
    "stock_price = 0\n",
    "buy_threshold = 0.9\n",
    "sold_threshold = 0.1\n",
    "transactions = []\n",
    "minute_count = 0\n",
    "for prediction,  (index, row)  in zip(predictions, aligned_test_data.iterrows()):\n",
    "    current_probability = prediction[0]\n",
    "    # print(current_probability)\n",
    "    # print(row['last_price'])\n",
    "    continue\n",
    "    if '.' in str(row['datetime']):\n",
    "        # 如果有小数点，分割为主时间部分和纳秒部分\n",
    "        time, nano_part = str(row['datetime']).split('.')\n",
    "    else:\n",
    "        # 如果没有小数点，则没有纳秒部分\n",
    "        time = str(row['datetime'])\n",
    "        nano_part = '0'\n",
    "    if is_time_in_ranges(datetime.strptime(time, \"%Y-%m-%d %H:%M:%S\")\n",
    ".time(),notrade_time):\n",
    "            continue\n",
    "    if current_probability is not None:\n",
    "        current_price = row['last_price']\n",
    "        buy1_price = row['bid_price1']\n",
    "        sell1_price = row['ask_price1']\n",
    "        minute_count = minute_count + 1\n",
    "            \n",
    "        if current_probability > buy_threshold and stock_quantity == 0 :\n",
    "            print(current_probability)\n",
    "            stock_quantity = funds // current_price\n",
    "            funds -= stock_quantity * current_price\n",
    "            fee = stock_quantity * 2\n",
    "            funds -= fee\n",
    "            stock_price = current_price\n",
    "            buy_price = current_price  # 记录买入价格\n",
    "            print(f\"Time: {row['datetime']} - Action: BUY at {current_price}, Quantity: {stock_quantity},Funds:{funds}\")\n",
    "            transactions.append({\n",
    "                'action': 'buy',\n",
    "                'time': row['datetime'],\n",
    "                'price': current_price,\n",
    "                'quantity': stock_quantity,\n",
    "                'funds_remaining': funds\n",
    "            })\n",
    "            minute_count = 0\n",
    "        elif minute_count > 100 and stock_quantity > 0 and current_probability<sold_threshold :\n",
    "            print(current_probability)\n",
    "            funds += stock_quantity * current_price\n",
    "            price_diff = current_price - buy_price  # 计算价格差异\n",
    "            # 判断价格差异是涨、跌还是平\n",
    "            if price_diff > 0:\n",
    "                direction = '涨'\n",
    "            elif price_diff < 0:\n",
    "                direction = '跌'\n",
    "            else:\n",
    "                direction = '平'\n",
    "            print(f\"Time: {row['datetime']} - Action: SELL at {current_price}, Quantity: {stock_quantity},Funds:{funds}, Price Change: {direction}\")\n",
    "            transactions.append({\n",
    "                'action': 'sell',\n",
    "                'time': row['datetime'],\n",
    "                'price': current_price,\n",
    "                'quantity': stock_quantity,\n",
    "                'funds_remaining': funds,\n",
    "                'price_change': direction\n",
    "            })\n",
    "            stock_quantity = 0\n",
    "            buy_price = 0  # 重置买入价格为0\n",
    "\n",
    "    \n",
    "if len(transactions)>0:\n",
    "    transactions_df = pd.DataFrame(transactions)\n",
    "    print(transactions_df['price_change'].value_counts())\n",
    "transactions_df.to_csv('transactions_tick_lstm.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
